{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:12:22.747850: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from gpt.trainer import (Trainer, TrainerConfig,à)\n",
    "from gpt.modeling import (GPT, GPT1Config,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token level GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(tf.data.Dataset):\n",
    "    def _gen_examples_from(data: List[int], block_size: int):\n",
    "        def _gen():\n",
    "            nb_examples = len(data)-block_size\n",
    "            for idx in range(nb_examples):\n",
    "                # grab a chunk of (block_size + 1) characters from the data\n",
    "                chunk = data[idx:idx + block_size + 1]\n",
    "                x = tf.convert_to_tensor(chunk[:-1])\n",
    "                y = tf.convert_to_tensor(chunk[1:])\n",
    "                yield x, y\n",
    "        return _gen\n",
    "\n",
    "    def __new__(\n",
    "        cls, input_ids: List[int], block_size: int, batch_size: int\n",
    "    ):\n",
    "        # nb_examples = len(data)-block_size\n",
    "        dataset =  (\n",
    "            tf.data.Dataset.from_generator(\n",
    "                cls._gen_examples_from(input_ids, block_size),\n",
    "                output_signature=(\n",
    "                    tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                    tf.TensorSpec(shape=(block_size,), dtype=tf.int32))\n",
    "                )\n",
    "                # .shuffle(nb_examples, reshuffle_each_iteration=True)\n",
    "                .batch(batch_size, drop_remainder=True)\n",
    "                .repeat()\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=128\n",
    "BATCH_SIZE=512\n",
    "\n",
    "def encode_text_to_ids(tokenizer, text: str):\n",
    "    output = tokenizer.encode(text)\n",
    "    return output.ids\n",
    "\n",
    "pretrained_tokenizer = Tokenizer.from_file(\"./data/tokenizer.json\")\n",
    "vocab_size = pretrained_tokenizer.get_vocab_size()\n",
    "\n",
    "text = open(\"./data/tinyshakespeare.txt\").read()\n",
    "input_ids = encode_text_to_ids(pretrained_tokenizer, text)\n",
    "train_dataset = TokenizedDataset(\n",
    "    input_ids, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    ")\n",
    "nb_examples = len(input_ids)-BLOCK_SIZE\n",
    "nb_optimization_steps = nb_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18146"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301842"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_optimization_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number optimization steps = 1178\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=2\n",
    "LEARNING_RATE=0.003#6.25e-5\n",
    "\n",
    "total_number_optimization_steps = nb_optimization_steps * EPOCHS\n",
    "\n",
    "print(f\"total number optimization steps = {total_number_optimization_steps}\")\n",
    "\n",
    "config = GPT1Config(\n",
    "    vocab_size=vocab_size, block_size=BLOCK_SIZE,\n",
    "    n_layer=8, n_head=8, n_embd=512\n",
    ")\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,\n",
    "    do_lr_decay=True, warmup_ratio=0.3, cosine_decay_alpha=0.0,\n",
    "    total_number_optimization_steps=total_number_optimization_steps, log_every_steps=10,\n",
    "    ckpt_path='./logs', trial_id='shakespeare_token_level'\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 1178: loss 4.95311 - acc 21.46% - lr 0.000000: 100%|██████████| 1178/1178 [15:07<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model, train_dataset, total_number_optimization_steps, config=tconf\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God , O God ! The city , my lord , and I is my good , I , To make his father ' ll the king , The lord ! My son , But , And thou art you . GLOUCESTER : I do I shall have my lord ! The rest , I say in my mother , The lord to his life of his father , To see me , Which have be the world ' s this ; Which\n"
     ]
    }
   ],
   "source": [
    "context = \"O God, O God!\"\n",
    "x = tf.convert_to_tensor(pretrained_tokenizer.encode(context).ids, dtype=tf.int32)[None, ...]\n",
    "y = model.sample(x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = pretrained_tokenizer.decode(y)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char level GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(tf.data.Dataset):\n",
    "    @staticmethod\n",
    "    def compute_vocab_from_text(text: str):\n",
    "        chars = sorted(list(set(text)))\n",
    "        data_size, vocab_size = len(text), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        return stoi, itos\n",
    "\n",
    "    def _gen_examples_from(text: str, block_size: int):\n",
    "        def _gen():\n",
    "            stoi, _ = CharDataset.compute_vocab_from_text(text)\n",
    "            nb_examples = len(text)-block_size\n",
    "            for idx in range(nb_examples):\n",
    "                # grab a chunk of (block_size + 1) characters from the data\n",
    "                chunk = text[idx:idx + block_size + 1]\n",
    "                # encode every character to an integer\n",
    "                dix = [stoi[s] for s in chunk]\n",
    "                x = tf.convert_to_tensor(dix[:-1])\n",
    "                y = tf.convert_to_tensor(dix[1:])\n",
    "                yield x, y\n",
    "        return _gen\n",
    "\n",
    "    def __new__(\n",
    "        cls, text: str, block_size: int, batch_size: int\n",
    "    ):\n",
    "        # nb_examples = len(text)-block_size\n",
    "        dataset =  (\n",
    "            tf.data.Dataset.from_generator(\n",
    "                cls._gen_examples_from(text, block_size),\n",
    "                output_signature=(\n",
    "                    tf.TensorSpec(shape=(block_size,), dtype=tf.int32),\n",
    "                    tf.TensorSpec(shape=(block_size,), dtype=tf.int32))\n",
    "                )\n",
    "                # .shuffle(nb_examples, reshuffle_each_iteration=True)\n",
    "                .batch(batch_size, drop_remainder=True)\n",
    "                .repeat()\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE=128\n",
    "BATCH_SIZE=512\n",
    "EPOCHS=2\n",
    "\n",
    "LEARNING_RATE=0.003#6.25e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 64 unique.\n"
     ]
    }
   ],
   "source": [
    "text = open(\"./data/tinyshakespeare.txt\").read()\n",
    "train_dataset_char = CharDataset(\n",
    "    text, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n",
    ")\n",
    "nb_examples = len(text)-BLOCK_SIZE\n",
    "nb_optimization_steps = nb_examples//BATCH_SIZE\n",
    "total_number_optimization_steps = nb_optimization_steps*EPOCHS\n",
    "\n",
    "stoi, itos = CharDataset.compute_vocab_from_text(text)\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 64\n",
      "no. examples = 1115265\n",
      "no. optimization steps = 2178\n",
      "no. total optimization steps = 4356\n"
     ]
    }
   ],
   "source": [
    "print(f\"vocab size = {len(stoi)}\")\n",
    "print(f\"no. examples = {nb_examples}\")\n",
    "print(f\"no. optimization steps = {nb_optimization_steps}\")\n",
    "print(f\"no. total optimization steps = {total_number_optimization_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 64 unique.\n",
      "tf.Tensor(\n",
      "[[17 46 55 ... 57 52  1]\n",
      " [46 55 56 ... 52  1 41]\n",
      " [55 56 57 ...  1 41 46]\n",
      " ...\n",
      " [ 5  1 57 ...  0 60 45]\n",
      " [ 1 57 45 ... 60 45 52]\n",
      " [57 45 42 ... 45 52 49]], shape=(512, 128), dtype=int32) tf.Tensor(\n",
      "[[46 55 56 ... 52  1 41]\n",
      " [55 56 57 ...  1 41 46]\n",
      " [56 57  1 ... 41 46 42]\n",
      " ...\n",
      " [ 1 57 45 ... 60 45 52]\n",
      " [57 45 42 ... 45 52 49]\n",
      " [45 42  1 ... 52 49 42]], shape=(512, 128), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset_char:\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4356 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 64 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 12:16:03.626045: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "step 2176: loss 1.69441 - acc 40.61% - lr 0.000567:  50%|████▉     | 2177/4356 [17:26<17:05,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 64 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 4354: loss 1.82917 - acc 44.29% - lr 0.000000: 100%|█████████▉| 4355/4356 [34:32<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 64 unique.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 4356: loss 1.65761 - acc 44.30% - lr 0.000000: 100%|██████████| 4356/4356 [34:33<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "config_char = GPT1Config(\n",
    "    vocab_size=vocab_size, block_size=BLOCK_SIZE,\n",
    "    n_layer=8, n_head=8, n_embd=512\n",
    ")\n",
    "tconf_char = TrainerConfig(\n",
    "    max_epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,\n",
    "    do_lr_decay=True, warmup_ratio=0.3, cosine_decay_alpha=0.0,\n",
    "    total_number_optimization_steps=total_number_optimization_steps, log_every_steps=10,\n",
    "    ckpt_path='./logs', trial_id='shakespeare_token_level'\n",
    ")\n",
    "\n",
    "model_char = GPT(config_char)\n",
    "\n",
    "trainer_char = Trainer(\n",
    "    model_char, train_dataset_char, total_number_optimization_steps, config=tconf_char\n",
    ")\n",
    "\n",
    "trainer_char.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! thou clep hurle of hysure of,\n",
      "For that heave, fault the when boy of the btoth?\n",
      "\n",
      "BRNOLAN:\n",
      "No brings? and with thereful not of mastand\n",
      "The pasit boy, that belive father: slack.\n",
      "\n",
      "KING RALY CORNOMHERDND II:\n",
      "As to his dest in extech'd tear the citys hold\n",
      "Why, would a fair beging his with brood:\n",
      "Ime have mind, and to from in my mortor:\n",
      "Here to servent may, and ascountage is strive is mercy\n",
      "I do the cousing so be hence\n",
      "Who bown with fractice one of my sands:\n",
      "And of weet this hatharge.\n",
      "\n",
      "DY ANRY:\n",
      "And to sempoke backs:\n",
      "O, that this soons bats haition sovight,\n",
      "The sucherence shallest as incred the pride and my crreasond:\n",
      "With they strause and with waye sin abistion\n",
      "Freep that so desin me the doth our husbmit\n",
      "Age be have is which othan one, seem the servest:\n",
      "My loves feeky say the to he, split,\n",
      "What than a the come me of mighter more be was their to here think of in\n",
      "Angen, with be had angeranly of the deserved\n",
      "And the head the for wreturn be her will had of whichier:\n",
      "Their foals be streagy of here havest\n",
      "Thy brhy says of sir, his well that him his we here asts,\n",
      "And thee a folds as to mostion by beggr a price,\n",
      "My doung humself\n",
      "Beith, to the will attent will bose minto then,\n",
      "If then it to betwent for as thoughts to sworrower.\n",
      "\n",
      "DUKE:\n",
      "Which hither, forth:\n",
      "I shall swort see\n",
      "Angy and mareform'd tain blust makes to by spast;\n",
      "And be wI art they woo thing from thuse of the come our strue-suison;\n",
      "At but will hand wife is not in the stand\n",
      "With of the be arms the did father, by the deat,\n",
      "The son,-gin my ment wife, stire me it beanst and from bring him at to steet alorts,\n",
      "The there obeing them here the fanger their the we all,\n",
      "And with a soume is show thy to britter.\n",
      "In an allent impeter we sain inlow it the drenge in arms\n",
      "The morringlyss and all thice touch and letters,\n",
      "Shus? what, and fielloss of that both in\n",
      "To streeks for shory lord say of me so trous my\n",
      "And hand, and we day you death these in me hated's call:\n",
      "When sempess is breased well mistaling;\n",
      "Thou had how to would my clifes on t\n"
     ]
    }
   ],
   "source": [
    "context = \"O God, O God!\"\n",
    "x = tf.convert_to_tensor([stoi[c] for c in context], dtype=tf.int32)[None, ...]\n",
    "y = model_char.sample(x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = \"\".join([itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('tfgpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfdc0eac67cf196f2171871a77a37aff362885343dc84e4d9326d459b303568d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
